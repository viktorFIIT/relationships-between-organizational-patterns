{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis of optimalization algorithms for Softmax Classification Neural Network (Multi-layer perceptron)**\n",
    "\n",
    "This model is designed to classify attributes of the patterns / pattern languages or sequences to a set of the classes and serves better than high-level API in Keras / Tensorflow, because we can better explain what our neural network is basically doing. Class with a highest probability is a prediction for a given sample in training dataset.\n",
    "\n",
    "Analysis results:\n",
    "- best optimizer so far is Adam devised in this work: https://arxiv.org/pdf/1412.6980.pdf\n",
    "- RMSprop is second\n",
    "- SGD is last but can be enhanced with SGDMomentum\n",
    "\n",
    "Because dataset of the attributes is still being consulted, sample dataset with 11 independent variables is provided. Dependent variable is a label, class. There are 10 classes in a dataset, classes 5, 6 and 7 are most present (some patterns are used more often than the others).\n",
    "\n",
    "Currently these options are being considered:\n",
    "\n",
    "- term frequencies (tf)\n",
    "- inverse frequencies (tf-idf)\n",
    "- probabilities (from our work Modelling of Organizational Pattern Sequences in Bayesian Network)\n",
    "- Table 1 from section 3.3. in http://www2.fiit.stuba.sk/~vranic/pub/ExtractingRelations.pdf\n",
    "\n",
    "Other unresolved (implementation) problems:\n",
    "\n",
    "- current implementation fails to work with categorical entropy and 10 output neurons, but works well with sparse categorical crossentropy. Usual cause for this is that dependent variable (pattern sequence encoded with number) is not one-hot encoded. See what one-hot encoding means.\n",
    "- 2 hidden layers, one of them with 350 neurons is still a lot for this small dataset\n",
    "- work with smaller batches\n",
    "\n",
    "<ins>Example how to interpret output from this Neural network</ins>:\n",
    "\n",
    "Let's say prediction for a first row in our training dataset is a vector of values: (0.08568677, 0.09945365, 0.08751229, 0.09474804, 0.1098659 , 0.12171782, 0.10679027, 0.10450635, 0.10343555, 0.08628327). This means first row in dataset has been assigned to class 6 because of its highest probability (0.1211782). Class 6 can represent organizational pattern, organizational pattern language or sequence of organizational patterns.\n",
    "\n",
    "Please note that prediction is correct if it is consistent with actual class. Accuracy of the model is one of the metrics used to evaluate this behavior.\n",
    "\n",
    "Theory behind this can be found in a book Dive into Deep Learning: https://d2l.ai/chapter_linear-networks/softmax-regression.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utils import Module\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "frequencies = pd.read_csv('dataset.csv', sep = ';')\n",
    "train, val, test = np.split(frequencies.sample(frac=1, random_state=42), [int(.6*len(frequencies)), int(.8*len(frequencies))])\n",
    "\n",
    "class Linear(Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(Linear, self).__init__()\n",
    "        # vahy by nemali byt nahodne ale 1/n nasobok vah co pomaha stabilizacii ucenia\n",
    "        self.W = np.random.randn(out_features, in_features)\n",
    "        self.b = np.zeros((out_features, 1))\n",
    "\n",
    "    def forward(self, input: np.ndarray) -> np.ndarray:\n",
    "        self.aPred = input # aktivacia predchadzajucej vrstvy je sucastou cache pre spatny prechod\n",
    "        self.m = self.aPred.shape[0] # pocet vzoriek je tiez sucastou cache pre spatny prechod\n",
    "        net = np.matmul(self.W, input) + self.b\n",
    "        return net\n",
    "\n",
    "    def backward(self, dz: np.ndarray) -> np.ndarray:\n",
    "        # dW a db su gradienty. dw udava smer narastania chyb. Pri zostupe potrebujeme ist proti smeru gradientu\n",
    "        self.dW = (1.0/self.m) * np.sum(np.matmul(dz, self.aPred.transpose((0,2,1))), axis=0) # aktivaciu si vrstva drzi aj pri spatnom prechode\n",
    "        self.db = (1.0/self.m) * np.sum(dz, axis=0)\n",
    "        # vrat vstup pre dalsi spatny prechod dalsou vrstvou. Ten sa vynasobi derivaciou aktivacnej funkcie\n",
    "        return np.matmul(self.W.transpose(), dz) # alebo return self.W.T @ dz, vysledkom bude vektor, spatne sirenie\n",
    "\n",
    "    def get_optimizer_context(self):\n",
    "        return [[self.W, self.dW], [self.b, self.db]]\n",
    "\n",
    "    def update_parameters(self, params): # commit 4ddc896096c5b12af45b0328ac399de5be54b88e\n",
    "        self.W, self.b = params\n",
    "        \n",
    "#------------------------------------------------------------------------------\n",
    "#   SigmoidActivationFunction class\n",
    "#------------------------------------------------------------------------------\n",
    "class Sigmoid(Module):\n",
    "    def __init__(self):\n",
    "        super(Sigmoid, self).__init__()\n",
    "\n",
    "    def forward(self, input: np.ndarray) -> np.ndarray:\n",
    "        self.fw_input = input\n",
    "        return 1.0 / (1.0 + np.exp(-input))\n",
    "\n",
    "    def backward(self, da) -> np.ndarray:\n",
    "        a = self(self.fw_input)\n",
    "        return np.multiply(da, np.multiply(a, 1 - a))\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "#   HyperbolicTangentActivationFunction class\n",
    "#------------------------------------------------------------------------------\n",
    "class Tanh(Module):\n",
    "    def __init__(self):\n",
    "        super(Tanh, self).__init__()\n",
    "\n",
    "    def forward(self, input: np.ndarray) -> np.ndarray:\n",
    "        self.fw_input = input\n",
    "        return (np.exp(2 * input) - 1) / (np.exp(2 * input) + 1)\n",
    "\n",
    "    def backward(self, da) -> np.ndarray:\n",
    "        a = self(self.fw_input)\n",
    "        return np.multiply(da, 1 - np.square(a))\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "#   Model class\n",
    "#------------------------------------------------------------------------------\n",
    "class Model(Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "    def forward(self, input) -> np.ndarray:\n",
    "        for name, module in self.modules.items():\n",
    "            input = module(input)\n",
    "        return input\n",
    "\n",
    "    def backward(self, z: np.ndarray):\n",
    "        for name, module in reversed(self.modules.items()):\n",
    "            z = module.backward(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "#   MeanSquareErrorLossFunction class\n",
    "#------------------------------------------------------------------------------\n",
    "class MSELoss(Module):\n",
    "    def __init__(self, reduce=\"mean\"):\n",
    "        super(MSELoss, self).__init__()\n",
    "        if reduce == \"mean\":\n",
    "            self.reduce_fn = np.mean\n",
    "        elif reduce == \"sum\":\n",
    "            self.reduce_fn = np.sum\n",
    "        elif reduce is None:\n",
    "            # return identity (do nothing)\n",
    "            self.reduce_fn = lambda x : x\n",
    "        else:\n",
    "            raise AttributeError\n",
    "\n",
    "    def forward(self, input: np.ndarray, target: np.ndarray) -> np.ndarray:\n",
    "        return self.reduce_fn(np.mean(np.power(target - input, 2), axis=0, keepdims=True))\n",
    "\n",
    "    def backward(self, input: np.ndarray, target: np.ndarray) -> np.ndarray:\n",
    "        return np.mean(-2 * (target - input), axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "#   BinaryCrossEntropyLossFunction class\n",
    "#------------------------------------------------------------------------------\n",
    "class BCELoss(Module):\n",
    "    def __init__(self, reduce=\"mean\"):\n",
    "        super(BCELoss, self).__init__()\n",
    "        if reduce == \"mean\":\n",
    "            self.reduce_fn = np.mean\n",
    "        elif reduce == \"sum\":\n",
    "            self.reduce_fn = np.sum\n",
    "        elif reduce is None:\n",
    "            # return identity (do nothing)\n",
    "            self.reduce_fn = lambda x : x\n",
    "        else:\n",
    "            raise AttributeError\n",
    "\n",
    "    def forward(self, input: np.ndarray, target: np.ndarray) -> np.ndarray:\n",
    "        # toto je v podstate mozne napisat aj ako\n",
    "        # - (np.multiply(target, np.log(input)) + np.multiply((1-target, np.log(1-input)))\n",
    "        return self.reduce_fn(-(target * np.log(input) + np.multiply((1 - target), np.log(1 - input))))\n",
    "\n",
    "    def backward(self, input: np.ndarray, target: np.ndarray) -> np.ndarray:\n",
    "        # derivacia loss funkcie podla aktivacie je -y/a + (1-y)/(1-a) kde y je target a \"a\" je input\n",
    "        # spatny prechod podla 2. prednasky v tomto semestri\n",
    "        return -np.divide(target, input) + np.divide(1 - target, 1 - input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "#   AbstractOptimizer class\n",
    "#------------------------------------------------------------------------------\n",
    "class Optimizer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def step(self):\n",
    "        raise NotImplemented\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "# StochasticGradientDescentOptimizer class\n",
    "# objekt tejto triedy realizuje jeden krok zostupu\n",
    "# zostup robi pri kazdom riadku datasetu\n",
    "# tento optimalizator je nevhodny pre vela prechodov / epoch\n",
    "# riesenim je nasekat dataset na davky\n",
    "#------------------------------------------------------------------------------\n",
    "class SGD(Optimizer):\n",
    "    def __init__(self, model:Model, lr:float):\n",
    "        super(SGD, self).__init__()\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "        self.context = {}\n",
    "\n",
    "    def step(self):\n",
    "        for name, layer in self.model.modules.items():\n",
    "            if hasattr(layer, 'get_optimizer_context'):\n",
    "                params = layer.get_optimizer_context()\n",
    "                if params is not None:\n",
    "                    [[W, dW],[b,db]] = params\n",
    "                    W = W - self.lr * dW # povodna hodnota - nasobok rychlosti ucenia*gradient\n",
    "                    b = b - self.lr * db\n",
    "                    layer.update_parameters([W,b])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "# SGDMomentumOptimizer class\n",
    "# ma tendenciu ist priamociarejsie ist k minimu ako SGD hore\n",
    "#------------------------------------------------------------------------------\n",
    "class SGDMomentum(Optimizer):\n",
    "    def __init__(self, model, lr, beta): #dopln svoje\n",
    "        super(SGDMomentum, self).__init__()\n",
    "        self.model = model\n",
    "        self.context = {}\n",
    "        # >>>> start_solution\n",
    "        self.lr = lr\n",
    "        self.beta = beta\n",
    "        # <<<< end_solution\n",
    "\n",
    "    def step(self):\n",
    "        for name, layer in self.model.modules.items():\n",
    "            if hasattr(layer, 'get_optimizer_context'):\n",
    "                params = layer.get_optimizer_context()\n",
    "                if params is not None:\n",
    "                    [[W, dW],[b,db]] = params\n",
    "                    if name in self.context.keys(): #dictionary pre rychlosti pravdepodobne, ak uz mas rychlost tak ju pouzi, inak vytvor\n",
    "                    # >>>> start_solution\n",
    "                        self.context[name][\"Vdw\"] = (1 - self.beta) * dW + self.beta * self.context[name][\"Vdw\"]\n",
    "                        self.context[name][\"Vdb\"] = (1 - self.beta) * db + self.beta * self.context[name][\"Vdb\"]\n",
    "                        #pass\n",
    "                    else:#nie su rychlosti v kontexte, takze su nula\n",
    "                        self.context[name] = {}\n",
    "                        self.context[name][\"Vdw\"] = (1 - self.beta) * dW\n",
    "                        self.context[name][\"Vdb\"] = (1 - self.beta) * db\n",
    "                        \n",
    "                        #pass\n",
    "                    W = W - self.lr*self.context[name][\"Vdw\"]\n",
    "                    b = b - self.lr*self.context[name][\"Vdb\"]\n",
    "                    # <<<< end_solution\n",
    "                    layer.update_parameters([W,b])\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "# RMSpropOptimizer class\n",
    "# vznikol na prednaske Geoffreyho Hintona\n",
    "# manipuluje s dlzkou kroku na ceste k optimalnym parametrom v zavislosti od velkosti gradientov\n",
    "#------------------------------------------------------------------------------\n",
    "class RMSprop(Optimizer): #podobne ako SGDMomentum\n",
    "    def __init__(self, model, lr, beta, epsilon):\n",
    "        super(RMSprop, self).__init__()\n",
    "        self.model = model\n",
    "        self.context = {}\n",
    "        # >>>> start_solution\n",
    "        self.lr = lr\n",
    "        self.beta = beta\n",
    "        self.epsilon = epsilon\n",
    "        # <<<< end_solution\n",
    "\n",
    "    def step(self):\n",
    "        for name, layer in self.model.modules.items():\n",
    "            if hasattr(layer, 'get_optimizer_context'):\n",
    "                params = layer.get_optimizer_context()\n",
    "                if params is not None:\n",
    "                    [[W, dW], [b, db]] = params\n",
    "                    if name in self.context.keys():\n",
    "                        # vzorce zo slajdu c. 81 prednasky Lecture3_NN_improvements\n",
    "                        self.context[name][\"Sdw\"] = (1 - self.beta) * np.square(dW) + self.beta * self.context[name][\"Sdw\"]\n",
    "                        self.context[name][\"Sdb\"] = (1 - self.beta) * np.square(db) + self.beta * self.context[name][\"Sdb\"]\n",
    "                        #pass\n",
    "                    else:\n",
    "                        self.context[name] = {}\n",
    "                        self.context[name][\"Sdw\"] = (1 - self.beta) * np.square(dW)\n",
    "                        self.context[name][\"Sdb\"] = (1 - self.beta) * np.square(db)\n",
    "                        #pass\n",
    "                        \n",
    "                    # tam kde mame maly gradient zvacsime dlzku kroku\n",
    "                    # tam kde mame velky gradient zmensime dlzku kroku\n",
    "                    # pri kazdom vzostupe pocitame podiel gradientu vah a druhej odmocniny Sdw (pozri vzorec)\n",
    "                    W = W - self.lr * (dW / (np.sqrt(self.context[name][\"Sdw\"]) + self.epsilon))\n",
    "                    # pri kazdom vzostupe pocitame podiel gradientu biasu a druhej odmocniny Sdb (pozri vzorec)\n",
    "                    b = b - self.lr * (db / (np.sqrt(self.context[name][\"Sdb\"]) + self.epsilon))\n",
    "                    layer.update_parameters([W, b])\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "# AdamOptimizer class\n",
    "# algoritmus z knihy Deep Learning od Iana Goodfellowa et al.\n",
    "#------------------------------------------------------------------------------\n",
    "class Adam(Optimizer):\n",
    "    def __init__(self, model, lr, beta1, beta2, epsilon):\n",
    "        super(Adam, self).__init__()\n",
    "        self.model = model\n",
    "        self.context = {}\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.lr = lr     \n",
    "        self.epsilon = epsilon\n",
    "        self.iteration = 0\n",
    "\n",
    "    def step(self):\n",
    "        # premenna iteration znamena timestamp v algoritme na str. 2 v praci Kingmu et al.\n",
    "        # pozri: https://www.researchgate.net/publication/269935079_Adam_A_Method_for_Stochastic_Optimization\n",
    "        self.iteration = self.iteration + 1\n",
    "        # <<<<<< until here\n",
    "        for name, layer in self.model.modules.items():\n",
    "            if hasattr(layer, 'get_optimizer_context'):\n",
    "                params = layer.get_optimizer_context()\n",
    "                if params is not None:\n",
    "                    [[W, dW], [b, db]] = params\n",
    "                    if name in self.context.keys(): \n",
    "                        # >>>> start_solution\n",
    "                        # first_moment_estimate = beta1*first_moment_estimate + (1-beta1)*gradientVah \n",
    "                        self.context[name][\"Vdw\"] = (1 - self.beta1) * dW + self.beta1 * self.context[name][\"Vdw\"]\n",
    "                        # second_moment_estimate = beta2*second_moment_estimate + (1-beta2)*gradientVah**2\n",
    "                        self.context[name][\"Sdw\"] = (1 - self.beta2) * np.square(dW) + self.beta2 * self.context[name][\"Sdw\"]\n",
    "                        # first_moment_estimate = beta1*first_moment_estimate + (1-beta1)*gradientBiasu\n",
    "                        self.context[name][\"Vdb\"] = (1 - self.beta1) * db + self.beta1 * self.context[name][\"Vdb\"]\n",
    "                        # second_moment_estimate = beta2*second_moment_estimate + (1-beta2)*gradientBiasu**2\n",
    "                        self.context[name][\"Sdb\"] = (1 - self.beta2) * np.square(db) + self.beta2 * self.context[name][\"Sdb\"]\n",
    "                        #pass\n",
    "                    else:\n",
    "                        self.context[name] = {}\n",
    "                        self.context[name][\"Vdw\"] = (1 - self.beta1) * dW\n",
    "                        self.context[name][\"Sdw\"] = (1 - self.beta2) * np.square(dW)\n",
    "\n",
    "                        self.context[name][\"Vdb\"] = (1 - self.beta1) * db\n",
    "                        self.context[name][\"Sdb\"] = (1 - self.beta2) * np.square(db)\n",
    "                        #pass\n",
    "                    # compute bias-corrected first and second raw moment estimates    \n",
    "                    Vdw_corr = self.context[name][\"Vdw\"] / (1 - self.beta1**self.iteration)\n",
    "                    Vdb_corr = self.context[name][\"Vdb\"] / (1 - self.beta1**self.iteration)\n",
    "                    Sdw_corr = self.context[name][\"Sdw\"] / (1 - self.beta2**self.iteration)\n",
    "                    Sdb_corr = self.context[name][\"Sdb\"] / (1 - self.beta2**self.iteration)\n",
    "                    \n",
    "                    W = W - self.lr * (Vdw_corr / (np.sqrt(Sdw_corr) + self.epsilon))\n",
    "                    b = b - self.lr * (Vdb_corr / (np.sqrt(Sdb_corr) + self.epsilon))\n",
    "                    # <<<< end_solution\n",
    "                    layer.update_parameters([W, b])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import gradient_check\n",
    "loss_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "mlpForSGD = Model()\n",
    "mlpForSGD.add_module(Linear(2, 3), 'Dense_1')\n",
    "mlpForSGD.add_module(Tanh(), 'Tanh_1')\n",
    "mlpForSGD.add_module(Linear(3, 4), 'Dense_2')\n",
    "mlpForSGD.add_module(Tanh(), 'Tanh_2')\n",
    "mlpForSGD.add_module(Linear(4, 5), 'Dense_3')\n",
    "mlpForSGD.add_module(Tanh(), 'Tanh_3')\n",
    "mlpForSGD.add_module(Linear(5, 1), 'Dense_4_out')\n",
    "mlpForSGD.add_module(Sigmoid(), 'Sigmoid') # najcastejsie je sigmoid posledna vrstva\n",
    "loss_fn = MSELoss(reduce='mean')\n",
    "\n",
    "mlpForAdam = Model()\n",
    "mlpForAdam.add_module(Linear(2, 3), 'Dense_1')\n",
    "mlpForAdam.add_module(Tanh(), 'Tanh_1')\n",
    "mlpForAdam.add_module(Linear(3, 4), 'Dense_2')\n",
    "mlpForAdam.add_module(Tanh(), 'Tanh_2')\n",
    "mlpForAdam.add_module(Linear(4, 5), 'Dense_3')\n",
    "mlpForAdam.add_module(Tanh(), 'Tanh_3')\n",
    "mlpForAdam.add_module(Linear(5, 1), 'Dense_4_out')\n",
    "mlpForAdam.add_module(Sigmoid(), 'Sigmoid')\n",
    "\n",
    "mlpForSGDMomentum = Model()\n",
    "mlpForSGDMomentum.add_module(Linear(2, 3), 'Dense_1')\n",
    "mlpForSGDMomentum.add_module(Tanh(), 'Tanh_1')\n",
    "mlpForSGDMomentum.add_module(Linear(3, 4), 'Dense_2')\n",
    "mlpForSGDMomentum.add_module(Tanh(), 'Tanh_2')\n",
    "mlpForSGDMomentum.add_module(Linear(4, 5), 'Dense_3')\n",
    "mlpForSGDMomentum.add_module(Tanh(), 'Tanh_3')\n",
    "mlpForSGDMomentum.add_module(Linear(5, 1), 'Dense_4_out')\n",
    "mlpForSGDMomentum.add_module(Sigmoid(), 'Sigmoid')\n",
    "\n",
    "mlpForRMSProp = Model()\n",
    "mlpForRMSProp.add_module(Linear(2, 3), 'Dense_1')\n",
    "mlpForRMSProp.add_module(Tanh(), 'Tanh_1')\n",
    "mlpForRMSProp.add_module(Linear(3, 4), 'Dense_2')\n",
    "mlpForRMSProp.add_module(Tanh(), 'Tanh_2')\n",
    "mlpForRMSProp.add_module(Linear(4, 5), 'Dense_3')\n",
    "mlpForRMSProp.add_module(Tanh(), 'Tanh_3')\n",
    "mlpForRMSProp.add_module(Linear(5, 1), 'Dense_4_out')\n",
    "mlpForRMSProp.add_module(Sigmoid(), 'Sigmoid')\n",
    "\n",
    "optimizerAdam = Adam(mlpForAdam, lr=0.001, beta1 = 0.9, beta2 = 0.999, epsilon = 10**(-8))\n",
    "\n",
    "N_epochs = 1000\n",
    "lossesForAdam = []\n",
    "for i in range(N_epochs):\n",
    "    epoch_loss = []\n",
    "    for mini_batch_X, mini_batch_Y in dataset:\n",
    "        # priamy prechod\n",
    "        predicted_Y_hat = mlpForAdam.forward(mini_batch_X) # predikcia\n",
    "        loss = loss_fn(predicted_Y_hat, mini_batch_Y) # strata pre kazdu vzorku\n",
    "        epoch_loss += [np.mean(loss)] # naklad pre kazdy mini batch\n",
    "        gradients = loss_fn.backward(predicted_Y_hat, mini_batch_Y)\n",
    "        # spatny prechod\n",
    "        mlpForAdam.backward(gradients)\n",
    "        # gradient_check(mlpForAdam, loss_fn, mini_batch_X, mini_batch_Y)\n",
    "        optimizerAdam.step()\n",
    "    lossesForAdam += [np.mean(epoch_loss)]\n",
    "    \n",
    "loss_dict['Adam'] = lossesForAdam\n",
    "\n",
    "optimizerSGD = SGD(mlpForSGD, lr=0.001)\n",
    "\n",
    "N_epochs = 1000\n",
    "lossesForSGD = []\n",
    "for i in range(N_epochs):\n",
    "    epoch_loss = []\n",
    "    for mini_batch_X, mini_batch_Y in dataset:\n",
    "        # priamy prechod\n",
    "        predicted_Y_hat = mlpForSGD.forward(mini_batch_X) # predikcia\n",
    "        loss = loss_fn(predicted_Y_hat, mini_batch_Y) # strata pre kazdu vzorku\n",
    "        epoch_loss += [np.mean(loss)] # naklad pre kazdy mini batch\n",
    "        gradients = loss_fn.backward(predicted_Y_hat, mini_batch_Y)\n",
    "        # spatny prechod\n",
    "        mlpForSGD.backward(gradients)\n",
    "        # gradient_check(mlpForSGD, loss_fn, mini_batch_X, mini_batch_Y)\n",
    "        optimizerSGD.step()\n",
    "    lossesForSGD += [np.mean(epoch_loss)]\n",
    "    \n",
    "loss_dict['SGD'] = lossesForSGD\n",
    "\n",
    "optimizerSGDMomentum = SGDMomentum(mlpForSGDMomentum, lr=0.001, beta=0.9)\n",
    "N_epochs = 1000\n",
    "lossesForSGDMomentum = []\n",
    "for i in range(N_epochs):\n",
    "    epoch_loss = []\n",
    "    for mini_batch_X, mini_batch_Y in dataset:\n",
    "        predicted_Y_hat = mlpForSGDMomentum.forward(mini_batch_X)\n",
    "        loss = loss_fn(predicted_Y_hat, mini_batch_Y)\n",
    "        epoch_loss += [np.mean(loss)]\n",
    "        gradients = loss_fn.backward(predicted_Y_hat, mini_batch_Y)\n",
    "        mlpForSGDMomentum.backward(gradients)\n",
    "        # gradient_check(mlpForSGDMomentum, loss_fn, mini_batch_X, mini_batch_Y)\n",
    "        optimizerSGDMomentum.step()\n",
    "    lossesForSGDMomentum += [np.mean(epoch_loss)]\n",
    "    \n",
    "loss_dict['SGDMomentum'] = lossesForSGDMomentum\n",
    "\n",
    "optimizerRMSProp = RMSprop(mlpForRMSProp, lr=0.001, beta=0.9, epsilon = 10**(-8))\n",
    "N_epochs = 1000\n",
    "lossesForRMSProp = []\n",
    "for i in range(N_epochs):\n",
    "    epoch_loss = []\n",
    "    for mini_batch_X, mini_batch_Y in dataset:\n",
    "        predicted_Y_hat = mlpForRMSProp.forward(mini_batch_X)\n",
    "        loss = loss_fn(predicted_Y_hat, mini_batch_Y)\n",
    "        epoch_loss += [np.mean(loss)]\n",
    "        gradients = loss_fn.backward(predicted_Y_hat, mini_batch_Y)\n",
    "        mlpForRMSProp.backward(gradients)\n",
    "        # gradient_check(mlpForRMSProp, loss_fn, mini_batch_X, mini_batch_Y)\n",
    "        optimizerRMSProp.step()\n",
    "    lossesForRMSProp += [np.mean(epoch_loss)]\n",
    "    \n",
    "loss_dict['RMSprop'] = lossesForRMSProp"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
