{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4781cee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\uzivatel\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n",
      "<>:52: DeprecationWarning: invalid escape sequence \\]\n",
      "c:\\users\\uzivatel\\appdata\\local\\programs\\python\\python38\\lib\\asyncio\\events.py:81: DeprecationWarning: `run_cell_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  self._context.run(self._callback, *self._args)\n",
      "<ipython-input-11-77877a176271>:52: DeprecationWarning: invalid escape sequence \\]\n",
      "  symbols = \"!\\\"#$%&()*+-./:;<=>?@[\\]^_`{|}~\\n\"\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\uzivatel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Corpus Text with out cleaning  127642\n",
      "Bag-of-Words vocabulary size:  1517\n",
      "\r",
      "Processing 870 combinations | Sampling itemset size 2\r",
      "Processing 5334 combinations | Sampling itemset size 3\r",
      "Processing 84 combinations | Sampling itemset size 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>support</th>\n",
       "      <th>itemsets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.063283</td>\n",
       "      <td>(Apprenticeship)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.218194</td>\n",
       "      <td>(ArchitectAlsoImplements)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.158866</td>\n",
       "      <td>(ArchitectControlsProduct)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.136454</td>\n",
       "      <td>(ArchitectureTeam)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.189189</td>\n",
       "      <td>(CodeOwnership)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>0.023731</td>\n",
       "      <td>(CodeOwnership, DeployAlongTheGrain, Stand-UpM...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>0.022413</td>\n",
       "      <td>(DevelopingInPairs, CodeOwnership, Stand-UpMee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>0.021753</td>\n",
       "      <td>(CodeOwnership, TeamPerTask, SacrificeOnePerson)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>0.023731</td>\n",
       "      <td>(CodeOwnership, TeamPerTask, Stand-UpMeeting)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>0.023072</td>\n",
       "      <td>(DevelopingInPairs, TeamPerTask, Stand-UpMeeting)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>226 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      support                                           itemsets\n",
       "0    0.063283                                   (Apprenticeship)\n",
       "1    0.218194                          (ArchitectAlsoImplements)\n",
       "2    0.158866                         (ArchitectControlsProduct)\n",
       "3    0.136454                                 (ArchitectureTeam)\n",
       "4    0.189189                                    (CodeOwnership)\n",
       "..        ...                                                ...\n",
       "221  0.023731  (CodeOwnership, DeployAlongTheGrain, Stand-UpM...\n",
       "222  0.022413  (DevelopingInPairs, CodeOwnership, Stand-UpMee...\n",
       "223  0.021753   (CodeOwnership, TeamPerTask, SacrificeOnePerson)\n",
       "224  0.023731      (CodeOwnership, TeamPerTask, Stand-UpMeeting)\n",
       "225  0.023072  (DevelopingInPairs, TeamPerTask, Stand-UpMeeting)\n",
       "\n",
       "[226 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for tf-idf\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "\n",
    "# ford LDA\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "import gensim.corpora as corpora\n",
    "from pprint import pprint\n",
    "import pyLDAvis.gensim\n",
    "import pickle \n",
    "import pyLDAvis\n",
    "import os\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "\n",
    "import nltk\n",
    "import os\n",
    "import sys\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "import numpy as np\n",
    "\n",
    "class CorpusReader:\n",
    "    data_dir=\"\"\n",
    "    all_data =[]\n",
    "    patterns = []\n",
    "    all_features = []\n",
    "    all_unigram_features = {}\n",
    "    all_bigram_features = {}\n",
    "    unigram_features_by_patterns={}\n",
    "    bigram_features_by_patterns={}\n",
    "    features_by_patterns = {}\n",
    "    words_documents={}\n",
    "    document_frequency={}\n",
    "    all_text = \"\";\n",
    "    totUni=0\n",
    "    totBi=0\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    s_words = stopwords.words('english')\n",
    "    s_words.extend(\n",
    "       ['...', '.', '.✥', '✥', '\\'', '\\\"','\\\",','.\\\"', ',', ':', '.**','(\\\"','\\\")', '\\\").','?\\\"','(', ')', 'e', '.,', '-', '....', ';', '[', ']', '—', '.)',\n",
    "        'therefor', '’', '([', '])','--','/','],','ff','e','g','bibref','60ksloc','foote2000'])\n",
    "    totalcorpuslenght=0;\n",
    "    def __init__(self,datadir):\n",
    "        self.data_dir=os.getcwd() + \"/\"+datadir+\"/\"\n",
    "        symbols = \"!\\\"#$%&()*+-./:;<=>?@[\\]^_`{|}~\\n\"\n",
    "        try:\n",
    "            for file in os.listdir(self.data_dir):\n",
    "                with open(self.data_dir+ file, 'r', encoding=\"utf-8\") as f:\n",
    "                    temp_text = \"\"\n",
    "                    for line in f:\n",
    "                        self.totalcorpuslenght+= len(line);\n",
    "                        for i in range(len(symbols)):\n",
    "                                line = np.str.replace(line, symbols[i], ' ')\n",
    "                                line = np.str.replace(line, \"  \", \" \")\n",
    "                        line= np.str.replace(line, \"'\", \"\")\n",
    "                        line = line.strip()\n",
    "                        if not line.isspace() and not line.startswith('\\n'):\n",
    "                            temp_text += line\n",
    "                            self.all_text += line\n",
    "\n",
    "                    filtered_text = [self.stemmer.stem(w) for w in nltk.wordpunct_tokenize(temp_text) if not w.lower() in self.s_words and len(w)>2]\n",
    "                    unigram_temp=nltk.FreqDist(filtered_text)\n",
    "                    filtered_bigram_text = nltk.bigrams(filtered_text)\n",
    "                    bigram_temp = nltk.FreqDist(filtered_bigram_text)\n",
    "                    uni_bi_temp = unigram_temp\n",
    "                    uni_bi_temp.update(bigram_temp)\n",
    "                    self.unigram_features_by_patterns[file.split('.')[0]] = unigram_temp\n",
    "                    self.bigram_features_by_patterns[file.split('.')[0]] = bigram_temp\n",
    "                    self.features_by_patterns[file.split('.')[0]] = uni_bi_temp\n",
    "                    self.patterns.append(file.split('.')[0])\n",
    "\n",
    "            filtered_text = [self.stemmer.stem(w) for w in nltk.wordpunct_tokenize(self.all_text) if not w.lower() in self.s_words]\n",
    "            filtered_bigram_text = nltk.bigrams(filtered_text)\n",
    "            self.all_unigram_features = nltk.FreqDist(filtered_text)\n",
    "            self.all_bigram_features = nltk.FreqDist(filtered_bigram_text)\n",
    "\n",
    "            for k, v in self.all_unigram_features.items():\n",
    "                for op in self.unigram_features_by_patterns:\n",
    "                    for ki, vi in self.unigram_features_by_patterns[op].items():\n",
    "                        if (k == ki):\n",
    "                            try:\n",
    "                                self.words_documents[k].add(op)\n",
    "                            except:\n",
    "                                self.words_documents[k] = {op}\n",
    "                                \n",
    "            for k, v in self.all_bigram_features.items():\n",
    "                for op in self.bigram_features_by_patterns:\n",
    "                    for ki, vi in self.bigram_features_by_patterns[op].items():\n",
    "                        if (k == ki):\n",
    "                            try:\n",
    "                                self.words_documents[k].add(op)\n",
    "                            except:\n",
    "                                self.words_documents[k] = {op}\n",
    "\n",
    "            for i in self.words_documents:\n",
    "                self.document_frequency[i]=len(self.words_documents[i])\n",
    "\n",
    "            for i,v in self.document_frequency.items():\n",
    "                if v>1:\n",
    "                    self.all_features.append(i)\n",
    "            print(\"Total Corpus Text with out cleaning \",self.totalcorpuslenght)\n",
    "            print(\"Bag-of-Words vocabulary size: \",len(self.all_features))\n",
    "        except IOError:\n",
    "            type, value, traceback = sys.exc_info()\n",
    "            print(\"Some errors occured : \" + type + \"\\n value: \" + value + \"\\n traceback: \" + traceback)\n",
    "\n",
    "    def getUniBigramDataUsingTFIDF(self,tfidfThreshold=0.1):\n",
    "        data =[]\n",
    "        data.append(self.patterns)\n",
    "\n",
    "        for feature in self.all_features:\n",
    "            term_appears =[]\n",
    "            df = self.document_frequency[feature]\n",
    "            idf = np.log10((len(self.patterns) + 1) / (df + 1))\n",
    "            for op, fs in self.features_by_patterns.items():\n",
    "                exists =0\n",
    "                try:\n",
    "                    opfeature = fs.get(feature)\n",
    "                    if opfeature !=None:\n",
    "                        tf = opfeature\n",
    "                        tfidf = tf * idf;\n",
    "                        if (tfidf >= tfidfThreshold):\n",
    "                            #print(feature,op,tf,df,round(idf,3),round(tfidf,3))\n",
    "                            exists=1\n",
    "                    term_appears.append(exists)\n",
    "                except:\n",
    "                    print('Something went wrong in building words to organizational patterns matrix')\n",
    "                    return\n",
    "            data.append(term_appears)\n",
    "        return data\n",
    "    def getWordsStatisticsFor(self, pattern1,pattern2):\n",
    "        data ={}\n",
    "        statistics ={}\n",
    "        p1unigramsize=0\n",
    "        p1bigramsize=0\n",
    "        p2unigramsize=0\n",
    "        p2bigramsize=0\n",
    "        unigramcommon=0\n",
    "        bigramcommon=0\n",
    "        totalcommon=0\n",
    "        for p1f,p1v in self.features_by_patterns.get(pattern1).items():\n",
    "            if(type(p1f)==tuple):\n",
    "                    p1bigramsize+=1\n",
    "            else:\n",
    "                    p1unigramsize+=1\n",
    "            for p2f,p2v in self.features_by_patterns.get(pattern2).items():\n",
    "                if(p1bigramsize==1 or p1bigramsize==1):\n",
    "                    if(type(p2f)==tuple):\n",
    "                        p2bigramsize+=1\n",
    "                    else:\n",
    "                        p2unigramsize+=1\n",
    "                if p1f==p2f:\n",
    "                    data[p1f]= [p1v,p2v]\n",
    "                    if(type(p1f)==tuple):\n",
    "                        bigramcommon+=1\n",
    "                    else:\n",
    "                        unigramcommon+=1\n",
    "        statistics[pattern1,'Unigrams']=p1unigramsize\n",
    "        statistics[pattern1,'Bigrams']=p1bigramsize\n",
    "        statistics[pattern1,'Total']=p1unigramsize+p1bigramsize\n",
    "        statistics[pattern2,'Unigrams']=p2unigramsize\n",
    "        statistics[pattern2,'Bigrams']=p2bigramsize\n",
    "        statistics[pattern2,'Total']=p2unigramsize+p2bigramsize\n",
    "        statistics['Common','Unigrams']=unigramcommon\n",
    "        statistics['Common','Bigrams']=bigramcommon\n",
    "        statistics['Common','Total']=unigramcommon+bigramcommon\n",
    "        return  data,statistics\n",
    "    def getWordsStatisticsFor(self, pattern1,pattern2,pattern3):\n",
    "        data ={}\n",
    "        statistics ={}\n",
    "        p1unigramsize=0\n",
    "        p1bigramsize=0\n",
    "        p2unigramsize=0\n",
    "        p2bigramsize=0\n",
    "        p3unigramsize=0\n",
    "        p3bigramsize=0\n",
    "        unigramcommon=0\n",
    "        bigramcommon=0\n",
    "        for p1f,p1v in self.features_by_patterns.get(pattern1).items():\n",
    "            if(type(p1f)==tuple):\n",
    "                    p1bigramsize+=1\n",
    "            else:\n",
    "                    p1unigramsize+=1\n",
    "            for p2f,p2v in self.features_by_patterns.get(pattern2).items():\n",
    "                if(p1unigramsize==1):\n",
    "                    if(type(p2f)==tuple):\n",
    "                        p2bigramsize+=1\n",
    "                    else:\n",
    "                        p2unigramsize+=1\n",
    "                if p1f==p2f:\n",
    "                    for p3f,p3v in self.features_by_patterns.get(pattern3).items():\n",
    "                        if(p1unigramsize==1):\n",
    "                            if(type(p3f)==tuple):\n",
    "                                p3bigramsize+=1\n",
    "                            else:\n",
    "                                p3unigramsize+=1\n",
    "                        if p2f==p3f:\n",
    "                            data[p1f]= [p1v,p2v,p3v]\n",
    "                            if(type(p1f)==tuple):\n",
    "                                bigramcommon+=1\n",
    "                            else:\n",
    "                                unigramcommon+=1\n",
    "\n",
    "        statistics[pattern1,'Unigrams']=p1unigramsize\n",
    "        statistics[pattern1,'Bigrams']=p1bigramsize\n",
    "        statistics[pattern1,'Total']=p1unigramsize+p1bigramsize\n",
    "        statistics[pattern2,'Unigrams']=p2unigramsize\n",
    "        statistics[pattern2,'Bigrams']=p2bigramsize\n",
    "        statistics[pattern2,'Total']=p2unigramsize+p2bigramsize\n",
    "        statistics[pattern3,'Unigrams']=p3unigramsize\n",
    "        statistics[pattern3,'Bigrams']=p3bigramsize\n",
    "        statistics[pattern3,'Total']=p3unigramsize+p3bigramsize\n",
    "        statistics['Common','Unigrams']=unigramcommon\n",
    "        statistics['Common','Bigrams']=bigramcommon\n",
    "        statistics['Common','Total']=unigramcommon+bigramcommon\n",
    "        return  data,statistics\n",
    "\n",
    "    def getStateistics(self,patterns,tfidfThreshold):\n",
    "        data={}\n",
    "        for feature in self.all_features:\n",
    "            term_appears =[]\n",
    "            df = self.document_frequency[feature]\n",
    "            idf = np.log10((len(self.patterns) + 1) / (df + 1))\n",
    "            exists = 1\n",
    "            tempfeature=[]\n",
    "            for p in patterns:\n",
    "                try:\n",
    "                    opfeature = self.features_by_patterns[p].get(feature)\n",
    "                    if opfeature !=None:\n",
    "                        tf = opfeature\n",
    "                        tfidf = tf * idf\n",
    "                        if (tfidf < tfidfThreshold):\n",
    "                            exists = 0\n",
    "                        tempfeature.append(round(tfidf,2))\n",
    "                except:\n",
    "                    print('Something went wrong in building words to organizational patterns matrix')\n",
    "                    return\n",
    "            if exists and len(tempfeature) > 2:\n",
    "                data[feature]=tempfeature\n",
    "        return data\n",
    "\n",
    "    def getNgramsExistence(self,patterns,tfidfThreshold):\n",
    "        data={}\n",
    "        for feature in self.all_features:\n",
    "            term_appears =[]\n",
    "            df = self.document_frequency[feature]\n",
    "            idf = np.log10((len(self.patterns) + 1) / (df + 1))\n",
    "\n",
    "            tempfeature=[]\n",
    "            for p in patterns:\n",
    "                exists = 0\n",
    "                try:\n",
    "                    opfeature = self.features_by_patterns[p].get(feature)\n",
    "                    if opfeature !=None:\n",
    "                        tf = opfeature\n",
    "                        tfidf = tf * idf\n",
    "                        if (tfidf >= tfidfThreshold):\n",
    "                            exists = 1\n",
    "                    tempfeature.append(exists)\n",
    "                except:\n",
    "                    print('Something went wrong in building words to organizational patterns matrix')\n",
    "                    return\n",
    "            data[feature]=tempfeature\n",
    "        return data\n",
    "    def getNgramsExistenceStatistics(self,patterns,tfidfThreshold):\n",
    "        data={}\n",
    "        for feature in self.all_features:\n",
    "            term_appears =[]\n",
    "            df = self.document_frequency[feature]\n",
    "            idf = np.log10((len(self.patterns) + 1) / (df + 1))\n",
    "            for p in patterns:\n",
    "                try:\n",
    "                    opfeature = self.features_by_patterns[p].get(feature)\n",
    "                    if opfeature !=None:\n",
    "                        tf = opfeature\n",
    "                        tfidf = tf * idf\n",
    "                        if (tfidf >= tfidfThreshold):\n",
    "                            try:\n",
    "                                data[p]=data[p]+1\n",
    "                            except:\n",
    "                                data[p]=1\n",
    "                except:\n",
    "                    print('Something went wrong in building words to organizational patterns matrix')\n",
    "                    return\n",
    "        return data\n",
    "\n",
    "\"\"\"\n",
    "problem and context sections of the Size The Organization organizational pattern and Surrogate Customer\n",
    "\"\"\"\n",
    "contexts = [\"\"\"within a larger organization, usually that of a sponsoring enterprise or company there need to be smaller organizations\n",
    "               capable of creating large software systems (greater than twenty-five thousand lines of code) that meet competitive cost\n",
    "               and schedule benchmarks. This pattern shows how the proper sizing of an organization is vital to the health of the project\n",
    "               and the productivity of its people. Large software projects (greater than twenty-five thousand lines of code) are seldom \n",
    "               delivered on time and within budget when the development team is too large or too small. There are two arguments that have\n",
    "               led us to this conclusion. There are limits to the size of software development teams that allow them to work effectively. \n",
    "               A team can handle a larger problem than an individual can. Adding people late to a project rarely helps complete that project\n",
    "               on time and within budget. If a software development team is too large, you can reach a point of greatly diminishing returns. \n",
    "               We have found empirically that an organization’s size affects a deliverable non-linearly. Communication overhead goes up as the\n",
    "               square of the size, which means that the organization becomes less cohesive as the square of the size while the “horsepower” of\n",
    "               the organization goes up only linearly. In addition, if the organization is too small, the team won’t have critical mass and \n",
    "               productivity will suffer. Projects larger than 25KSLOC can rarely be done by a SOLO VIRTUOSO (4.2.5) and overly small organizations have inadequate inertia and can easily become\n",
    "               unstable\"\"\", \n",
    "            \"\"\"It is important to exchange ideas and clarify issues with customers. But a customer may not be available. There are several \n",
    "            reasons that a customer may be unavailable. If the project is new, there may be no customers yet. In fact, the product might \n",
    "            even create its own customers. Even in existing products, the organization may never have established relationships with customers,\n",
    "            and now is not a propitious time to do so. In some cases, the customer might not have the time right now. They’re busy too. But you\n",
    "            need answers immediately. Some corporate cultures are such that the developers are insulated from the customers; they just don’t talk. \n",
    "            We certainly aren’t recommending it, but it does happen. Whatever the cause, there is a temptation for developers to make their best \n",
    "            guess and go on. The problem is that developers are naturally biased by their own designs, and will assume customer behavior that conforms \n",
    "            to their design. There are always other ways to think about the application, some of which may not mesh with the developer’s view\n",
    "            \"\"\"] \n",
    "\n",
    "df = pd.DataFrame({'context': ['context1', 'context2'], 'text' : contexts})\n",
    "\n",
    "tfidf = TfidfVectorizer(stop_words='english', norm=None)\n",
    "tfidf_matrix = tfidf.fit_transform(df['text'])\n",
    "df_dtm = pd.DataFrame(tfidf_matrix.toarray(),\n",
    "                      index=df['context'].values,\n",
    "                      columns=tfidf.get_feature_names_out())\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
    "        \n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) \n",
    "             if word not in stop_words] for doc in texts]\n",
    "        \n",
    "words = list(sent_to_words(contexts))\n",
    "data_words = remove_stopwords(words)\n",
    "id2word = corpora.Dictionary(data_words)\n",
    "\n",
    "texts = words\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "num_of_patterns = 2\n",
    "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                       id2word=id2word,\n",
    "                                       num_topics=num_of_patterns)\n",
    "\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "LDAvis_data_filepath = os.path.join('.ldavis_prepared_'+str(num_of_patterns))\n",
    "if 1 == 1:\n",
    "    LDAvis_prepared = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
    "    with open(LDAvis_data_filepath, 'wb') as f:\n",
    "        pickle.dump(LDAvis_prepared, f)\n",
    "with open(LDAvis_data_filepath, 'rb') as f:\n",
    "    LDAvis_prepared = pickle.load(f)\n",
    "pyLDAvis.save_html(LDAvis_prepared, './ldavis_prepared_'+ str(num_of_patterns) +'.html')\n",
    "\n",
    "\"\"\"\n",
    "neural network is designed to receive tf-idf frequencies of the words in textual descriptions of the problem and context\n",
    "section of organizational patterns. Trained neural network enables us to identify and distinguish those patterns which\n",
    "descriptions have a same tf-idf frequency of the words and thus assign them to the class 1 or 0.\n",
    "\n",
    "Frequencies for different tuple of the sections from pattern description can be used too. This example is set up for\n",
    "problem and context sections.\n",
    "\n",
    "Dependent variables:\n",
    "Y = binary indicator, 1.0 if word frequencies are from pattern language 1, 0 if from the other\n",
    "X1 = tf-idf of a word for the problem and context section of the pattern from pattern language 1\n",
    "X2 = tf-idf of a word for the problem and context section of the pattern from pattern language 2\n",
    "\"\"\"\n",
    "\n",
    "class Module:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.modules = OrderedDict()\n",
    "        self._parameters = OrderedDict()\n",
    "\n",
    "    def add_module(self, module, name:str):\n",
    "        if hasattr(self, name) and name not in self.modules:\n",
    "            raise KeyError(\"attribute '{}' already exists\".format(name))\n",
    "        elif '.' in name:\n",
    "            raise KeyError(\"module name can't contain \\\".\\\"\")\n",
    "        elif name == '':\n",
    "            raise KeyError(\"module name can't be empty string \\\"\\\"\")\n",
    "        self.modules[name] = module\n",
    "\n",
    "    def register_parameter(self, name, param):\n",
    "        if '.' in name:\n",
    "            raise KeyError(\"parameter name can't contain \\\".\\\"\")\n",
    "        elif name == '':\n",
    "            raise KeyError(\"parameter name can't be empty string \\\"\\\"\")\n",
    "        elif hasattr(self, name) and name not in self._parameters:\n",
    "            raise KeyError(\"attribute '{}' already exists\".format(name))\n",
    "        else:\n",
    "            self._parameters[name] = param\n",
    "\n",
    "    def parameters(self, recurse=True):\n",
    "        for name, param in self._parameters.items():\n",
    "            if param.requires_grad:\n",
    "                yield name, param\n",
    "        if recurse:\n",
    "            for name, module in self._modules.items():\n",
    "                for name, param in module.parameters(recurse):\n",
    "                    if param.requires_grad:\n",
    "                        yield name, param\n",
    "\n",
    "    def __dir__(self):\n",
    "        module_attrs = dir(self.__class__)\n",
    "        attrs = list(self.__dict__.keys())\n",
    "        modules = list(self._modules.keys())\n",
    "        parameters = list(self._parameters.keys())\n",
    "        keys = module_attrs + attrs + modules + parameters\n",
    "\n",
    "        # Eliminate attrs that are not legal Python variable names\n",
    "        keys = [key for key in keys if not key[0].isdigit()]\n",
    "\n",
    "        return sorted(keys)\n",
    "\n",
    "    def __getattr__(self, name: str):\n",
    "        if '_modules' in self.__dict__:\n",
    "            modules = self.__dict__['_modules']\n",
    "            if name in modules:\n",
    "                return modules[name]\n",
    "        if '_parameters' in self.__dict__:\n",
    "            parameters = self.__dict__['_parameters']\n",
    "            if name in parameters:\n",
    "                return parameters[name]\n",
    "        raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n",
    "            type(self).__name__, name))\n",
    "\n",
    "    def __setattr__(self, name, value):\n",
    "        if isinstance(value, Module):\n",
    "            self._modules[name] = value\n",
    "        elif isinstance(value, np.ndarray):\n",
    "            self.register_parameter(name, value)\n",
    "        else:\n",
    "            object.__setattr__(self, name, value)\n",
    "\n",
    "    def forward(self, *args, **kwargs) -> np.ndarray:\n",
    "        pass\n",
    "\n",
    "    def backward(self, *args, **kwargs) -> np.ndarray:\n",
    "        pass\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self.forward(*args, **kwargs)\n",
    "\n",
    "# beriem poslednu verziu funkcie gradient_check\n",
    "# naposledy menena v commite https://github.com/hudeclukas/neural_networks_at_fiit_2022/commit/baeacfa5a8ef83c7c7979b2010a0f5a4d336bf10\n",
    "def gradient_check(network:Module, loss_function:Module, X:np.ndarray, Y:np.ndarray, epsilon=1e-7):\n",
    "    # https: // datascience - enthusiast.com / DL / Improving_DeepNeural_Networks_Gradient_Checking.html\n",
    "    # Set-up variables\n",
    "    gradapprox = []\n",
    "    grad_backward = []\n",
    "\n",
    "    for name, layer in network.modules.items():\n",
    "        # Compute gradapprox\n",
    "        if not hasattr(layer, \"W\"):\n",
    "            continue\n",
    "        if not hasattr(layer, \"dW\"):\n",
    "            continue\n",
    "        shape = layer.W.shape\n",
    "        # print(shape[0], ',', shape[1])\n",
    "        for i in range(shape[0]):\n",
    "            for j in range(shape[1]):\n",
    "                # print('i',i,'j',j)\n",
    "                # Compute J_plus[i]. Inputs: \"parameters_values, epsilon\". Output = \"J_plus[i]\".\n",
    "                # \"_\" is used because the function you have to outputs two parameters but we only care about the first one\n",
    "                origin_W = np.copy(layer.W[i][j])\n",
    "\n",
    "                layer.W[i][j] = origin_W + epsilon\n",
    "                A_plus = network(X)\n",
    "                J_plus = np.mean(loss_function(A_plus, Y))\n",
    "\n",
    "                # Compute J_minus[i]. Inputs: \"parameters_values, epsilon\". Output = \"J_minus[i]\".\n",
    "                layer.W[i][j] = origin_W - epsilon\n",
    "                A_minus = network(X)\n",
    "                J_minus = np.mean(loss_function(A_minus, Y))\n",
    "\n",
    "                # Compute gradapprox[i]\n",
    "                gradapprox.append((J_plus - J_minus) / (2 * epsilon))\n",
    "                # print(layer.name, layer.dW.shape)\n",
    "                # grad = np.mean(layer.dW, axis=0, keepdims=True)\n",
    "                # grad_backward.append(grad[0][i][j])\n",
    "                grad_backward.append(layer.dW[i][j])\n",
    "                layer.W[i][j] = origin_W\n",
    "\n",
    "    # Compare gradapprox to backward propagation gradients by computing difference.\n",
    "    gradapprox = np.reshape(gradapprox, (-1, 1))\n",
    "    grad_backward = np.reshape(grad_backward, (-1, 1))\n",
    "\n",
    "    numerator = np.linalg.norm(grad_backward - gradapprox)\n",
    "    denominator = np.linalg.norm(grad_backward) + np.linalg.norm(gradapprox)\n",
    "    difference = numerator / denominator\n",
    "\n",
    "    if difference > 2e-7 or not difference:\n",
    "        print(\n",
    "            \"\\033[91m\" + \"There is a mistake in the backward propagation! difference = \" + str(difference) + \"\\033[0m\")\n",
    "    else:\n",
    "        print(\n",
    "            \"\\033[92m\" + \"Your backward propagation works perfectly fine! difference = \" + str(difference) + \"\\033[0m\")\n",
    "    \n",
    "\n",
    "class Linear(Module):\n",
    "     \n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(Linear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.W = np.random.randn(out_features, in_features)\n",
    "        self.dW = np.zeros_like(self.W)\n",
    "        self.b = np.zeros((out_features, 1)) # Watch-out for the shape\n",
    "        self.db = np.zeros_like(self.b)\n",
    "\n",
    "    def forward(self, input: np.ndarray) -> np.ndarray:\n",
    "        self.fw_inputs = input\n",
    "        self.m = self.fw_inputs.shape[1]\n",
    "        net = np.matmul(self.W, input) + self.b\n",
    "        return net\n",
    "\n",
    "    def backward(self, dz: np.ndarray) -> np.ndarray:\n",
    "        da = self.W.T @ dz\n",
    "        self.dW = (1/self.m)*np.matmul(dz, self.fw_inputs.T)\n",
    "        self.db = (1/self.m)*np.sum(dz, axis = 1, keepdims=True)\n",
    "        return da\n",
    "        \n",
    "\n",
    "class Sigmoid(Module):\n",
    "    def __init__(self):\n",
    "        super(Sigmoid, self).__init__()\n",
    "\n",
    "    \"\"\"\n",
    "    aktivacna funkcia v tomto tvare je uvedena v prezentacii Lecture1_history\n",
    "    na slajde c. 57\n",
    "    \"\"\"\n",
    "    def forward(self, input: np.ndarray) -> np.ndarray:\n",
    "        self.fw_input = input\n",
    "        return 1.0 / (1.0 + np.exp(-input))\n",
    "\n",
    "    \"\"\"\n",
    "    derivacia aktivacnej funkcie ma tvar f(x)*(1-f(x)) pricom f(x) je aktivacna\n",
    "    funkcia Sigmoid, uvedena vo funkcii forward tejto triedy\n",
    "    \"\"\"\n",
    "    def backward(self, da) -> np.ndarray:\n",
    "        z = self.forward(self.fw_input) # priamy prechod je tu mysleny ako aktivacia\n",
    "        # vynasob f(x) a 1-f(x) a potom vysledok vynasob gradientom aktivacie\n",
    "        # postup na toto je v prednaske Lecture2_NN_miniframework, 4.1. prvy vzorec zhora\n",
    "        # nad ohranicenim v cervenom, slide 122\n",
    "        return np.multiply(da, np.multiply(z, 1 - z)) \n",
    "\n",
    "\n",
    "class Tanh(Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Tanh, self).__init__()\n",
    "\n",
    "    \"\"\"\n",
    "    funkcny predpis pre funkciu forward je uvedeny v prezentacii Lecture1_history\n",
    "    na slajde c. 57\n",
    "    \"\"\"\n",
    "    def forward(self, input: np.ndarray) -> np.ndarray:\n",
    "        self.fw_input = input\n",
    "        return (np.exp(2*input) - 1) / (np.exp(2*input) + 1)\n",
    "\n",
    "    def backward(self, da) -> np.ndarray:\n",
    "        # vynasom derivaciu aktivacnej funkcie s gradientom aktivacie, vid. prednaska Lecture2_NN_miniframework, slide 122\n",
    "        return np.multiply(da, 1-np.square(self.forward(self.fw_input)))\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "#   RELUActivationFunction class\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "class ReLU(Module):\n",
    "    def __init__(self):\n",
    "        super(ReLU, self).__init__()\n",
    "\n",
    "    # funkcny predpis je v prezentacii Lecture1_history\n",
    "    # slide 58\n",
    "    def forward(self, input: np.ndarray) -> np.ndarray:\n",
    "        self.fw_input = input\n",
    "        return np.maximum(input, 0.0) \n",
    "\n",
    "    # derivacia pre Rectified Linear Unit je na slajde 58 v prednaske Lecture1_history\n",
    "    def backward(self, da) -> np.ndarray:\n",
    "        # vynasob derivaciu aktivacnej funkcie gradientom aktivacie, vid. prednaska Lecture2_NN_miniframework, slide 122\n",
    "        # derivacia akt. funkcie je v prednaske Lecture1_history, slide 58\n",
    "        return np.multiply(da, np.where(self.fw_input>0, 1, 0))\n",
    "\n",
    "class MSELoss(Module):\n",
    "    def __init__(self):\n",
    "        super(MSELoss, self).__init__()\n",
    "\n",
    "    def forward(self, input: np.ndarray, target: np.ndarray) -> np.ndarray:\n",
    "        return np.square(np.subtract(target, input)).mean()\n",
    "\n",
    "    def backward(self, input: np.ndarray, target: np.ndarray) -> np.ndarray:\n",
    "        return (-2*(target-input))\n",
    "\n",
    "\n",
    "class BCELoss(Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(BCELoss, self).__init__()\n",
    "\n",
    "    def forward(self, input: np.ndarray, target: np.ndarray) -> np.ndarray:\n",
    "        return -(np.multiply(target, np.log(input))+np.multiply((1-target), np.log(1-input)))\n",
    "\n",
    "    def backward(self, input: np.ndarray, target: np.ndarray) -> np.ndarray:\n",
    "        return -target / input  + (1 - target) / (1 - input)\n",
    "    \n",
    "\n",
    "class Model(Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "    def forward(self, input) -> np.ndarray:\n",
    "        for name, module in self.modules.items():\n",
    "            input = module(input)\n",
    "        return input\n",
    "\n",
    "    def backward(self, z: np.ndarray):\n",
    "        for name, module in reversed(self.modules.items()):\n",
    "            z = module.backward(z)\n",
    "        return z\n",
    "    \n",
    "\n",
    "def tf_idf_from_contexts(): # for two patterns: Size The Organization and Surrogate Customer organizational pattern\n",
    "    \n",
    "    # frequencies of words 'assurance', 'benchmarks', 'blindsided' and 'budget'    \n",
    "    X = np.array([[0.0, 1.405465, 0.0, 1.405], [2.81, 0.0, 1.405, 0.0]])\n",
    "    # two classes, 0 if Organization Design Patterns, 1 if Piecemeal Growth Pattern Language\n",
    "    # first three words are from Organization Design Patterns (STO)\n",
    "    # last one is from Piecemeal Growth Pattern Language (SC)\n",
    "    Y = np.array([[0., 0., 0., 1.]])\n",
    "    return X, Y\n",
    "\n",
    "dataset_features_X, dataset_labels_Y = tf_idf_from_contexts()\n",
    "\n",
    "\n",
    "# using BCE\n",
    "mlp = Model()\n",
    "mlp.add_module(Linear(2, 20), 'first-hidden')\n",
    "mlp.add_module(Sigmoid(), 'activation-1')\n",
    "mlp.add_module(Linear(20, 20), 'second-hidden')\n",
    "mlp.add_module(Sigmoid(), 'activation-2')\n",
    "mlp.add_module(Linear(20, 20), 'third-hidden')\n",
    "mlp.add_module(Tanh(), 'activation-3')\n",
    "mlp.add_module(Linear(20, 20), 'fourth-hidden')\n",
    "mlp.add_module(ReLU(), 'activation-4')\n",
    "mlp.add_module(Linear(20, 20), 'fifth-hidden')\n",
    "mlp.add_module(Sigmoid(), 'activation-5')\n",
    "mlp.add_module(Linear(20, 1), 'sixth-hidden')\n",
    "mlp.add_module(Sigmoid(), 'activation-6')\n",
    "\n",
    "y_hat = mlp.forward(dataset_features_X)\n",
    "\n",
    "bce = BCELoss()\n",
    "loss = bce.forward(y_hat, dataset_labels_Y)\n",
    "\n",
    "back = bce.backward(y_hat, dataset_labels_Y)\n",
    "\n",
    "output = mlp.backward(back)\n",
    "\n",
    "# (gradient_check(mlp, bce, dataset_features_X, dataset_labels_Y))\n",
    "\n",
    "\"\"\"\n",
    "This visualization is a modifier version of the one designed by https://nlp.stanford.edu/events/illvi2014/papers/sievert-illvi2014.pdf\n",
    "\n",
    "LDAvis consists from two interactive components, intertopic distance map on the left and bar chart on the right. Intertopic distance\n",
    "map is a visualization of the topics in 2D space. Circles are plotted using a multi-dimensional scaling algorithm, text with many\n",
    "dimensions is squeezed to a 2D space. Circles represent patterns descriptions. Circles close together mean descriptions of the patterns\n",
    "are similar. Pattern description consists of many topics. Pattern description and the topic is not the same.\n",
    "\n",
    "When you click on a topic in the intertopic distance map, bar chart changes to display 30 (by-default) most salient words included in the pattern\n",
    "(circle) description. Most salient words are the most informative words across the whole dataset representing a specific topic. Ligth bars represent \n",
    "salient terms. Darker bars show frequency of those terms which are topic-specific.\n",
    "\n",
    "Because dark bars for Size The Organization (any value of Lambda) almost totally eclipse light bars this means terms used in its description\n",
    "nearly exclusively belong to this pattern.\n",
    "\n",
    "Adjusting Lambda close to 0 highlights potentially rare and exclusive terms for a specific organizational pattern (and its topic).\n",
    "\n",
    "Summary: visualization displays\n",
    "\"\"\"\n",
    "# LDAvis_prepared\n",
    "\n",
    "op = CorpusReader(\"corpus\")\n",
    "\n",
    "data = op.getUniBigramDataUsingTFIDF(tfidfThreshold=0.3)\n",
    "columns = data.pop(0)\n",
    "df = pd.DataFrame(data=data, columns=columns)\n",
    "\n",
    "freq_patterns = apriori(df, min_support=0.02, use_colnames=True, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19981ff6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fdff9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
