library(tidyverse)

tweets <- read_csv('data/covid19_tweets.csv') # z readr (tidyverse), vracia tibble

patterns <- read.delim("data/patterns.txt", sep="@")
patterns <- patterns[, -1]

View(patterns)

install.packages("RWeka")
install.packages("ggsci")
install.packages("magick", verbose=TRUE)







head(patterns, n = 1L)

tweets$user_verified=ifelse(tweets$user_verified=="TRUE",1,0)

glimpse(tweets)

head(tweets, n = 1L)

str_detect(tweets$hashtags, "COVID")

(real <- tweets %>% 
  filter(user_verified == 1))

# ak tweety maju hashtagy a neobsahuju
# COVID|CORONA|VIRUS|LOCKDOWN|PANDEMIC|TOGETHER|DISTANC|FACE|MASK|REMOTE|REOPEN|TEST, 
# tak mozu byt fake
fake2a <- tweets %>%
  filter(!is.na(hashtags)) %>%
  filter(str_detect(hashtags, regex("COVID|CORONA|VIRUS|LOCKDOWN|PANDEMIC|TOGETHER|DISTANC|FACE|MASK|REMOTE|REOPEN|TEST", ignore_case = TRUE), negate = TRUE))

View(fake2a)

# ak hashtagy obsahuju slovo COVID a ak obsahuju minimalne 50 slov
# tak tiez mozu byt fake
fake2b <- tweets %>%
  filter(!is.na(hashtags)) %>%
  filter(str_detect(hashtags, regex("COVID", ignore_case = TRUE), negate = FALSE)) %>%
  filter(str_detect(hashtags, regex("CORONA|VIRUS|LOCKDOWN|PANDEMIC|TOGETHER|DISTANC|FACE|MASK|REMOTE|REOPEN|TEST", ignore_case = TRUE), negate = TRUE)) %>%
  filter(nchar(hashtags)>50)

#toto su tweety, ktore sice obsahuju COVID, ale okrem toho vela dalsich nesuvisiacich hashtagov nemajucich nic spolocne s COVID-om
fake8 <- tweets %>%
  filter(!is.na(hashtags)) %>%
  filter(str_detect(hashtags, regex("COVID|CORONA|VIRUS|LOCKDOWN|PANDEMIC|TOGETHER|DISTANC", ignore_case = TRUE), negate = TRUE))
fake8 <- tweets %>%
  filter(!is.na(hashtags)) %>%
  filter(str_detect(hashtags, regex("COVID|CORONA|VIRUS|LOCKDOWN|PANDEMIC|TOGETHER|DISTANC", ignore_case = TRUE), negate = TRUE))
#toto su tweety, ktore nemaju nic spolocne s COVID-om


# filter co odstrani prazdne zaznamy
faketest3 <- subset(tweets,!is.na(tweets$hashtags))

# alebo
faketest4 <- tweets %>%
  filter(!is.na(hashtags))

fake6 <- tweets %>% 
  filter(str_detect(hashtags, regex("COVID|CORONA|VIRUS|LOCKDOWN|PANDEMIC|TOGETHER|DISTANC", ignore_case = TRUE), negate = TRUE))

fake <- tweets %>% 
  filter(hashtags %in% c("Twitter for iPhone", "Twitter for Android", "Twitter Web App"))

faketest <- tweets %>% 
  filter(str_contains(hashtags, c("COVID","COVID19","CORONA","VIRUS","LOCKDOWN","TOGETHER"), logic = "not", ignore.case = TRUE))

faketest4 <- tweets %>% 
  filter(str_contains(hashtags, "COVID"))

faketest8 <- filter(tweets, str_contains(tweets$hashtags, "graphicdesigner"))

str_contains("['light']", "COVID")

faketest3 <- subset(tweets,!is.na(tweets$hashtags))

str_contains("['logo', 'graphicdesigner', 'logodesign', 'logodesinger', 'icon', 'minimalist', 'Abstract']
", c("COVID","COVID19","CORONA","VIRUS","LOCKDOWN","TOGETHER"), logic = "not", ignore.case = TRUE)
str_contains("['COVID19', 'TamilNadu', 'chennai']", c("COVID","COVID19","CORONA","VIRUS","LOCKDOWN","TOGETHER"), logic = "not", ignore.case = TRUE)
str_contains("['light']", c("COVID","COVID19","CORONA","VIRUS","LOCKDOWN","TOGETHER"), logic = "not", ignore.case = TRUE)

str_contains("abc", c("a", "b", "e"), logic = "or")

str_contains("abc", c("a", "b", "e"), logic = "not")

smaller <- tweets %>% 
  filter(source %in% c("Twitter for iPhone", "Twitter for Android", "Twitter Web App"))

# hypoteza H1: mame informacie o pouzivateloch ktori su schopni vyrazne viac sirit tvrdenia?
# odpoved na hypotezu H1:

# existuje par jedincov co maju vyrazne viac followerov ako ostatni
# tito vsak maju menej priatelov ako vacsina ktora az tak vela
# followerov ako tato skupinka nema ale zato ma viacej priatelov
# hviezdy vela priatelov nemaju...bez ohladu na pouzivane zariadenie
# potom su neverifikovani useri s vyrazne viac priatelmi

# pozriet:
# https://r4ds.had.co.nz/data-visualisation.html

to_readable_scientific_form <- function(label) {
  label <- format(label, scientific = TRUE)
  label <- gsub("^(.*)e", "'\\1'e", label)
  label <- gsub("e", "%*%10^", label)
  return (parse(text=label))
}

ggplot(data = smaller) + 
  geom_point(mapping = aes(x = user_followers, y = user_friends)) + 
  geom_smooth(mapping = aes(x = user_followers, y = user_friends)) +
  scale_y_continuous(labels=to_readable_scientific_form) +
  scale_x_continuous(labels=to_readable_scientific_form) +
  facet_wrap(~ source, nrow = 2) + 
  ggsave("graphs/vztah-medzi-followermi-a-priatelmi.pdf") # https://r4ds.had.co.nz/data-visualisation.html#facets


# hypoteza H2: je rozlozenie frekvencie tweetov v ramci okresu rovnake?
# odpoved na hypotezu h2: nie je

tweets$matches <- grepl(pattern = "(NY USA)$", tweets$user_location)
tweets <- tweets[tweets$matches == TRUE, ]

bar <- ggplot(data = tweets) + 
  geom_bar(
    mapping = aes(x = user_location, fill = user_location), 
    show.legend = FALSE,
    width = 1
  ) + 
  theme(aspect.ratio = 1) +
  labs(x = NULL, y = NULL)

bar + coord_polar()  + ggsave("graphs/rozlozenie-okresu-ny.pdf")

tweets <- read_csv('data/covid19_tweets.csv') # z readr (tidyverse), vracia tibble
tweets$user_verified=ifelse(tweets$user_verified=="TRUE",1,0)

# hypoteza H3: je prevazna vacsina neverifikovanych userov alebo viacej hviezd?
# odpoved na hypotezu H3: vacsina je neverifikovanych
tweets$user_verified <- as.factor(tweets$user_verified)
ggplot(tweets, aes(x = user_verified, fill = user_verified)) + 
  geom_bar() +
  theme_classic() +
  theme(axis.title = element_text(face = 'bold', size = 15),
        axis.text = element_text(size = 13)) +
  theme(legend.position = 'none') + ggsave("graphs/vacsina-neverifikovanych.pdf")

# hypoteza H4: je najcastejsie pouzivana webova verzia Twitteru?
# odpoved na hypotezu H4: ano je, za nou su najpouzivanejsie zariadenia s OS Android 
tweets$source <- with(tweets, reorder(source, source, function(x) length(x)))
sources_plot <- ggplot(data = tweets) +
  ggtitle("podiel zdrojov odkial tweety pochadzaju") +
  geom_bar(aes(x= source)) + coord_flip()
sources_plot %+% subset(tweets, source %in% c("Twitter for iPhone", "Twitter for Android", "Twitter Web App", "TweetDeck")) + ggsave("graphs/najpouzivanejsia-platforma.pdf")

# zastupenie vybranych diskutujucich v debate
tweets$user_name <- with(tweets, reorder(user_name, user_name, function(x) length(x)))
ggplot(tweets, aes(x = user_name, fill = user_verified)) +
  geom_bar() +
  theme_classic() +
  theme(axis.title = element_text(face = 'bold', size = 15),
        axis.text = element_text(size = 13, angle = 90))

# ake hashtagy sa najviac pouzivaju?
tweets$hashtags <- as.factor(tweets$hashtags)
tweets %>%
  group_by(hashtags) %>%
  count() %>%
  arrange(desc(n))

# ake sa najmenej?
tweets %>%
  group_by(hashtags) %>%
  count() %>%
  arrange((n))

# analyza "pocitov" pouzivatelov pri pisani tweetov

# hypoteza H5: vzhladom na diskutovanu temu je viacej negativnych komentarov?
# odpoved na hypotezu H5: ano, negativnych spojeni je o 769 viac ako tych pozitivnych
library(tidytext)
tweets <- read_csv('data/covid19_tweets.csv')
# spravime si zoznam slov z tweetov
words <- tibble(tweet = tweets$text) %>% unnest_tokens(word, tweet)
# porovname ho so zoznamom slov (Bing je meno autora) s priradenym sentimentom
words %>%
  inner_join(get_sentiments("bing")) %>% # zoznam slov s priradenym sentimentom
  count(sentiment) %>% # pocitanie statistik v tabulke nizsie sa deje tu
  spread(sentiment, n, fill = 0) %>% 
  mutate(sentiment = positive - negative)

# negativne a pozitivne slova v tweetoch su takmer vyrovnane,
# pozitivnych je vsak o 769 menej ako tych negativnych

# ako sa menila nalada na Twitteri casom? 

GetSentiment <- function(tweet){
  tokens <- data_frame(text = tweet) %>% unnest_tokens(word, text)
  sentiment <- tokens %>%
    inner_join(get_sentiments("bing")) %>%
    count(sentiment) %>% 
    spread(sentiment, n, fill = 0)
  return (sentiment)
}

library(tidytext)

ngramator <- function(tweet) { 
  return (lapply(ngrams(words(tweet), numberOfGrams), paste, collapse = " "))
}

numberOfGrams = 1

size <- length(tweets$text)
sentiments <- data.frame("year", "negative", "positive", "sentiment", "place", "majority", "followers", "friends", "tweet", "source", "user", "hashtagsno", "numberOfWords")
for(i in 1:size){
  s <- GetSentiment(tweets[[3]][[i]])
  if (!is.null(s$negative) && !is.null(s$positive)) { 
    sentiments[i, 1] = format(as.Date(tweets[[9]][[i]], format="%Y-%m-%d"),"%Y")
    sentiments[i, 2] = s$negative
    sentiments[i, 3] = s$positive
    sentiments[i, 4] = s 
    sentiments[i, 5] = tweets[[2]][[i]]
    sentiments[i, 6] = s$negative - s$positive
    sentiments[i, 7] = tweets[[5]][[i]]
    sentiments[i, 8] = tweets[[6]][[i]]
    sentiments[i, 9] = tweets[[10]][[i]] 
    sentiments[i, 10] = tweets[[12]][[i]] 
    sentiments[i, 11] = tweets[[1]][[i]]
    sentiments[i, 12] = lengths(gregexpr("[(')]", tweets[[11]][[i]])) / 2
    sentiments[i, 13] = length(ngramator(tweets[[10]][[i]]))
  }
}

# pozrime sa na to ci sme si vygenerovali nejake NA hodnoty
# install.packages('Amelia')
# install.packages('mlbench')
library(Amelia)
library(mlbench)
missmap(sentiments, col=c("blue", "red"), legend=FALSE)
sentiments <- na.omit(sentiments)
sentiments <- filter(sentiments, X.hashtagsno. > 1)
sentiments = sentiments[-1,] # odstranime hlavicku

# hypoteza H6: existuju v datasete tweety s gramatickymi chybami?
# odpoved na hypotezu H6: ano, v datasete sa nachadzaju aj spojene slova,
# ktore by sme skor cakali ako hashtagy
spojene_slova <- data_frame(tweet = sentiments$X.tweet.) %>% 
  unnest_tokens(word, tweet) %>% 
  group_by(word) %>% 
  count(word, sort = TRUE) %>% mutate(len=nchar(word)) %>% filter(len > 15)

ggplot(head(spojene_slova, 20), aes(x=reorder(word, -n), y=n)) + 
  geom_col() + 
  theme_light() + 
  ylab("Pocetnost takeho slova") + 
  xlab("Najpouzivanejsie slova") + 
  coord_flip() +
  ggtitle("Pocetnost slov v tweete dlhsich (>15)") + ggsave('graphs/najpouzivanejsie-spojene-slova.pdf')

# oanotujeme si tweety, zavedieme premennu 'narrative'
sentiments2 <- sentiments %>% mutate(group = ifelse((grepl("laboratory", sentiments$X.tweet.)==TRUE & sentiments$X.majority. > 0) | (grepl("cases", sentiments$X.tweet.)==TRUE & sentiments$X.majority. > 0),"europe-narrative",
                                                    ifelse((grepl("market", sentiments$X.tweet.)==TRUE & sentiments$X.majority. > 0) | (grepl("coronavirus", sentiments$X.tweet.)==TRUE & sentiments$X.majority. > 0),"china-narrative",
                                                           ifelse((grepl("putin", sentiments$X.tweet.)==TRUE & sentiments$X.majority. > 0) | (grepl("people", sentiments$X.tweet.)==TRUE & sentiments$X.majority. > 0),"politic", "undetermined"))))

# na zaradenie tweetu do European narrative pozadujeme aby v nom bolo viac negativnych
# ako pozitivnych slov a aby obsahoval slova 'laboratory' a 'cases'

# hypoteza H7: vieme pri vsetkych tweetoch oznacenych ako European narrative odhalit ich zdroj?
# odpoved na hypotezu H7: ano
sentiments_european_narrative <- sentiments2 %>% filter((group == 'europe-narrative'))  %>% 
  group_by(X.source., group) %>% count(group) %>% filter(group !='undetermined')
with(sentiments_european_narrative, reorder(X.source., X.source., function(x) length(x)))
# cim viac dat v tabulke sentiments tym lepsi graf
ggplot(data = sentiments_european_narrative) +
  ggtitle("podiel zdrojov na European narrative") +
  geom_bar(aes(x= X.source.)) + coord_flip() + ggsave("graphs/podiel-zdrojov-na-european-narrative.pdf")

# prispevky ktorych diskutujucich sme tak vyhodnotili?
sentiments_european_narrative <- sentiments2 %>% filter((group == 'europe-narrative'))  %>% 
  group_by(X.source., group) %>% count(group, X.user.) %>% filter(group !='undetermined')
with(sentiments_european_narrative, reorder(X.source., X.source., function(x) length(x)))
ggplot(data = head(sentiments_european_narrative, 30)) +
  ggtitle("autori tweetov s European narrative") +
  geom_bar(aes(x= X.user.)) + coord_flip() +
  ggsave("graphs/autori-s-prispevkami-european-narrative.pdf")

# na zaradenie tweetu do Chinese narrative pozadujeme aby v nom bolo viac negativnych
# ako pozitivnych slov a aby obsahoval slova 'market' a 'coronavirus'

# hypoteza H7: vieme pri vsetkych tweetoch oznacenych ako Chinese narrative odhalit ich zdroj?
# ano
sentiments_china_narrative <- sentiments2 %>% filter((group == 'china-narrative'))  %>% 
  group_by(X.source., group) %>% count(group) %>% filter(group !='undetermined')
with(sentiments_china_narrative, reorder(X.source., X.source., function(x) length(x)))
ggplot(data = sentiments_china_narrative) +
  ggtitle("podiel zdrojov na China narrative") +
  geom_bar(aes(x=X.source.)) + coord_flip() + ggsave("graphs/podiel-zdrojov-na-chinese-narrative.pdf")

# ktori diskutujuci napisali prispevky oznacene ako chinese narrative?
sentiments_china_narrative <- sentiments2 %>% filter((group == 'china-narrative')) %>% 
  group_by(X.source., group) %>% filter(group !='undetermined')
with(sentiments_china_narrative, reorder(X.source., X.source., function(x) length(x)))
ggplot(data = head(sentiments_china_narrative, 100)) +
  ggtitle("Chinese narrative") +
  geom_bar(aes(x= sentiments_china_narrative$X.user.)) + coord_flip() +
  ggsave("graphs/autori-s-prispevkami-chinese-narrative.pdf")

sentiments_russia_narrative <- sentiments2 %>% filter((group == 'politic')) %>% 
  group_by(X.source., group) %>% filter(group !='undetermined')
with(sentiments_russia_narrative, reorder(X.source., X.source., function(x) length(x)))
ggplot(data = head(sentiments_russia_narrative, 100)) +
  ggtitle("Russian narrative") +
  geom_bar(aes(x= sentiments_russia_narrative$X.user.)) + coord_flip() +
  ggsave("graphs/autori-s-prispevkami-russian-narrative.pdf")

library(reshape2)
ggplot(melt(cat_freq_tri_df),aes(x =variable, y=value,fill = group)) + 
  geom_col(position = "dodge") + coord_flip() + xlab("trigramy") + ylab("pocetnost vyskytu trigramov v narrativoch") + 
  theme() + ggtitle("najpocetnejsie trigramy podla jednotlivych narrativov") + ggsave("graphs/najpocetnejsie-trigramy-podla-jednotlivych-narrativov.pdf")


(tweetsCorpus <- VCorpus(VectorSource(unlist(lapply(sentiments2$X.tweet., as.character))))) 
inspect(tweetsCorpus[1:4])
inspect(tweetsCorpus[[4]])
grepl("anxious", tweetsCorpus[[4]]$content) # existencia vyjadrenia nalad / pocitov
unigram_matrix <- DocumentTermMatrix(tweetsCorpus, control = list(tokenize = ngramator, stopwords = FALSE, stemming = TRUE))

set.seed(123)
n_obs <- nrow(sentiments2)
prop_split <- .75
training_index <- sample(1:n_obs, round(n_obs * prop_split))

tweet_dtm <- as.matrix(unigram_matrix)
y_true <- as.matrix(sentiments2$group)
x_train <- tweet_dtm[training_index, ]
x_test <- tweet_dtm[-training_index, ]

library(glmnet)
glm_fit <- glmnet(x_train , y_true[training_index], family = "multinomial")
predicted_glm <- predict(glm_fit, x_test, type = "class")
(accuracy_glm <- sum(y_true[-training_index] == predicted_glm)/ length(predicted_glm))

library(tm)

sentiments2 <- filter(sentiments2, X.friends. != 'NA' & X.hashtagsno. != 'NA' & X.friends. != 'NA' & X.negative. != 'NA' & X.positive. != 'NA' & X.numberOfWords. != 'NA')
        
plot(sentiments2$X.friends. ~ (sentiments2$X.hashtagsno. * sentiments2$X.friends.) + (sentiments2$X.negative. - sentiments2$X.positive.)*sentiments2$X.numberOfWords.)

library(reshape2)

ggplot(head(melt(cat_freq_tri_df), 80),aes(x =variable, y=value,fill = group)) + 
  geom_col(position = "dodge") + coord_flip() + xlab("trojice pouzivanych slov") + ylab("pocetnost vyskytu trigramov v narrativoch") + 
  theme() + ggtitle("najpocetnejsie trigramy") + ggsave("graphs/najpocetnejsie-trigramy-podla-jednotlivych-narrativov.pdf")

library(rstatix)

# nahodna premenna ktorou je pocetnost pozitivnych alebo negativnych
# slov v tweete nie je normalne rozdelena
shapiro_test(as.numeric(sentiments$X.positive.))
shapiro_test(as.numeric(sentiments$X.negative.))

library(fitdistrplus)
descdist(as.numeric(sentiments$X.negative.), discrete = TRUE)

# nenasli sme korelaciu medzi negativnymi a pozitivnymi vyjadreniami
# a poctom followerov a priatelov
cor.test(as.numeric(sentiments$X.negative.), as.numeric(sentiments$X.positive.))

ggplot(data = sentiments) +
  geom_bar(mapping = aes(x = cut))

head(sentiments, n = 1L)

sentimentz3 <- sentiments %>% 
  mutate(Neg = X.negative.)

sentimentz8 <- data.frame(matrix(ncol=5,nrow=17263))
sentimentz8$Neg <- sentiments$X.negative.
sentimentz8$Pos <- sentiments$X.positive.
sentimentz8$Diff <- sentiments$X.majority.
sentimentz8$Foll <- sentiments$X.followers.
sentimentz8$Frie <- sentiments$X.friends.

sentimentz8n <- as.data.frame(apply(sentimentz8, 2, as.numeric)) 
str(sentimentz8n)
attach(sentimentz8n)

pairs( ~ Neg + Pos + Diff + Foll + Frie, panel=function(x,y){
  points(x,y)
  abline(lm(y~x), col='red')})

panel.cor <- function(x,y, digits=2, prefix="", cex.cor){
  usr <- par("usr")
  on.exit(par(usr))
  par(usr = c(0,1,0,1))
  r <- abs(cor(x,y,use="complete.obs"))
  txt <- format(c(r,0.123456789), digits = digits)[1]
  txt <- paste0(prefix, txt,sep="")
  if(missing(cex.cor)) cex.cor <- 0.8/strwidth(txt)
  text(0.5, 0.5, txt, cex = cex.cor * (1 + r) / 2)
}

panel.hist <- function(x, ...){
  usr <- par("usr")
  on.exit(par(usr))
  par(usr = c(usr[1:2],0,1.5))
  h <- hist(x, plot = FALSE)
  breaks <- h$breaks
  nB <- length(breaks)
  y <- h$counts
  y <- y/max(y)
  rect(breaks[-nB], 0, breaks[-1], y, col="white",...)
}

panel.lm <- function(x,y,col = par("col"), bg = NA, pch = par("pch"),
                     cex = 1, col.smooth = "darkblue",...){
  points(x,y,pch=pch, col=col, bg=bg, cex=cex)
  abline(stats::lm(x ~ y), col = col.smooth, ...)
}    

attach(sentiments)

pairs( ~ Neg + Pos + Diff + Foll + Frie, 
       upper.panel=panel.cor, 
       diag.panel=panel.hist,
       lower.panel=panel.smooth)